{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    }
   ],
   "source": [
    "from aif360.algorithms.postprocessing.reject_option_classification import RejectOptionClassification\n",
    "from aif360.datasets import BinaryLabelDataset, AdultDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# TODO: change the import method\n",
    "import sys\n",
    "import os\n",
    "repo_root = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0, repo_root)\n",
    "repair_folder = os.path.join(repo_root, \"humancompatible\", \"repair\")\n",
    "sys.path.insert(0, repair_folder)\n",
    "from humancompatible.repair.cost import *\n",
    "from humancompatible.repair.coupling_utils import *\n",
    "from humancompatible.repair.data_analysis import *\n",
    "from humancompatible.repair.group_blind_repair import *\n",
    "from humancompatible.repair.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path=os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROCpostprocess:\n",
    "    def __init__(self,X_val,y_val,var_list,prediction_model,favorable_label):\n",
    "        self.X_val =X_val\n",
    "        self.y_val =y_val\n",
    "        self.model = prediction_model\n",
    "        self.positive_index = 1 # positive label\n",
    "        self.var_list = var_list\n",
    "        self.var_dim=len(self.var_list)\n",
    "        self.ROC = self.buildROCusingval()\n",
    "        self.favorable_label = favorable_label\n",
    "\n",
    "    def buildbinarydata(self,X,y):\n",
    "        df=pd.DataFrame(np.concatenate((X,y.reshape(-1,1)), axis=1),columns=self.var_list+['S','W','Y'])\n",
    "        binaryLabelDataset = BinaryLabelDataset(\n",
    "                            # favorable_label=self.favorable_label,\n",
    "                            # unfavorable_label=0,\n",
    "                            df=df[self.var_list+['S','W','Y']], #df_test.drop('X',axis=1), #[x_list+['S','W','Y']],\n",
    "                            label_names=['Y'],\n",
    "                            instance_weights_name=['W'],\n",
    "                            protected_attribute_names=['S'],\n",
    "                            privileged_protected_attributes=[np.array([1.0])],\n",
    "                            unprivileged_protected_attributes=[np.array([0.])])\n",
    "        return binaryLabelDataset,df\n",
    "\n",
    "    def buildROCusingval(self):\n",
    "        dataset_val = self.buildbinarydata(self.X_val,self.y_val)[0]\n",
    "        dataset_val_pred = dataset_val.copy(deepcopy=True)\n",
    "        dataset_val_pred.scores = self.model.predict_proba(dataset_val.features[:,0:self.var_dim])[:,self.positive_index].reshape(-1,1)\n",
    "        privileged_groups = [{'S': 1}]\n",
    "        unprivileged_groups = [{'S': 0}]\n",
    "        # Metric used (should be one of allowed_metrics)\n",
    "        metric_name = \"Statistical parity difference\"\n",
    "        # Upper and lower bound on the fairness metric used\n",
    "        metric_ub = 0.05\n",
    "        metric_lb = -0.05\n",
    "        ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups, \n",
    "                                        privileged_groups=privileged_groups, \n",
    "                                        low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "                                        num_class_thresh=50, num_ROC_margin=10,\n",
    "                                        metric_name=metric_name,\n",
    "                                        metric_ub=metric_ub, metric_lb=metric_lb)\n",
    "        ROC = ROC.fit(dataset_val, dataset_val_pred)\n",
    "        print(\"Optimal classification threshold (with fairness constraints) = %.4f\" % ROC.classification_threshold)\n",
    "        print(\"Optimal ROC margin = %.4f\" % ROC.ROC_margin)\n",
    "        return ROC\n",
    "\n",
    "    def postprocess(self,X_test,y_test,tv_origin): # the tv distance won't change\n",
    "        dataset_test_pred,df_test = self.buildbinarydata(X_test,y_test) #.copy(deepcopy=True)\n",
    "        dataset_test_pred.scores = self.model.predict_proba(X_test[:,0:self.var_dim])[:,self.positive_index].reshape(-1,1)\n",
    "        dataset_test_pred_transf = self.ROC.predict(dataset_test_pred)\n",
    "        y_pred = dataset_test_pred_transf.labels\n",
    "        # return dataset_test_pred_transf.convert_to_dataframe()[0]\n",
    "\n",
    "        di = DisparateImpact_postprocess(df_test,y_pred,favorable_label=self.favorable_label)\n",
    "        f1_macro = f1_score(df_test['Y'], y_pred, average='macro',sample_weight=df_test['W'])\n",
    "        f1_micro = f1_score(df_test['Y'], y_pred, average='micro',sample_weight=df_test['W'])\n",
    "        f1_weighted = f1_score(df_test['Y'], y_pred, average='weighted',sample_weight=df_test['W'])\n",
    "        new_row=pd.Series({'DI':di,'f1 macro':f1_macro,'f1 micro':f1_micro,'f1 weighted':f1_weighted,\n",
    "                           'TV distance':tv_origin,'method':'ROC'})\n",
    "        return new_row.to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projpostprocess:\n",
    "    \n",
    "    def __init__(self,X_test,y_test,x_list,var_list,prediction_model,K,e,thresh,favorable_label=1):\n",
    "        self.model = prediction_model\n",
    "        self.K=K\n",
    "        self.e=e\n",
    "        self.x_list=x_list\n",
    "        self.var_list=var_list\n",
    "        self.var_dim=len(var_list)\n",
    "        self.favorable_label = favorable_label\n",
    "\n",
    "        df_test=pd.DataFrame(np.concatenate((X_test,y_test.reshape(-1,1)), axis=1),columns=var_list+['S','W','Y'])\n",
    "        df_test=df_test.groupby(by=var_list+['S','Y'],as_index=False).sum()\n",
    "        if len(x_list)>1:\n",
    "            df_test['X'] = list(zip(*[df_test[c] for c in x_list]))\n",
    "            self.x_range=sorted(set(df_test['X']))\n",
    "            weight=list(1/(df_test[x_list].max()-df_test[x_list].min())) # because 'education-num' range from 1 to 16 while others 1 to 4\n",
    "            self.C=c_generate_higher(self.x_range,weight)\n",
    "        else:\n",
    "            df_test['X']=df_test[x_list]\n",
    "            self.x_range=sorted(set(df_test['X']))\n",
    "            self.C=c_generate(self.x_range)\n",
    "        self.df_test = df_test\n",
    "        self.var_range=list(pd.pivot_table(df_test,index=var_list,values=['S','W','Y']).index)\n",
    "        self.distribution_generator()\n",
    "        \n",
    "        if thresh == 'auto':\n",
    "            self.thresh_generator()\n",
    "        else:\n",
    "            self.thresh=thresh\n",
    "\n",
    "    def distribution_generator(self):\n",
    "        bin=len(self.x_range)\n",
    "        dist=rdata_analysis(self.df_test,self.x_range,'X')\n",
    "        \n",
    "        dist['v']=[(dist['x_0'][i]-dist['x_1'][i])/dist['x'][i] for i in range(bin)]\n",
    "        \n",
    "        dist['t_x']=dist['x'] # #dist['x'] #dist['x_0']*0.5+dist['x_1']*0.5 \n",
    "        self.px=np.matrix(dist['x']).T\n",
    "        self.ptx=np.matrix(dist['t_x']).T\n",
    "        if np.any(dist['x_0']==0): \n",
    "            self.p0=np.matrix((dist['x_0']+1.0e-9)/sum(dist['x_0']+1.0e-9)).T\n",
    "        else:\n",
    "            self.p0=np.matrix(dist['x_0']).T \n",
    "        if np.any(dist['x_1']==0):\n",
    "            self.p1=np.matrix((dist['x_1']+1.0e-9)/sum(dist['x_1']+1.0e-9)).T\n",
    "        else:\n",
    "            self.p1=np.matrix(dist['x_1']).T \n",
    "        self.V=np.matrix(dist['v']).T\n",
    "        self.tv_origin=sum(abs(dist['x_0']-dist['x_1']))/2\n",
    "        # return px,ptx,V,p0,p1\n",
    "    \n",
    "    def _run_method(self, method, C, eps, px, ptx, K, V=None, theta=None):\n",
    "        group_blind = GroupBlindRepair(C, px, ptx, V=V, epsilon=eps, K=K)\n",
    "        if method == \"baseline\":\n",
    "            group_blind.fit_baseline()\n",
    "        elif method == \"partial_repair\":\n",
    "            group_blind.fit_partial(theta)\n",
    "        elif method == \"total_repair\":\n",
    "            group_blind.fit_total()\n",
    "        return group_blind.coupling_matrix()\n",
    "\n",
    "    def coupling_generator(self,method,para=None):\n",
    "        if method == 'unconstrained':\n",
    "            coupling=self._run_method(method=\"baseline\", C=self.C, eps=self.e, px=self.px, ptx=self.ptx, K=self.K)\n",
    "        elif method == 'barycentre':\n",
    "            coupling=self._run_method(method=\"baseline\", C=self.C, eps=self.e, px=self.p0, ptx=self.p1, K=self.K)\n",
    "        elif method == 'partial':\n",
    "            coupling=self._run_method(method=\"partial_repair\", C=self.C, eps=self.e, px=self.px, ptx=self.ptx, V=self.V, theta=para, K=self.K)\n",
    "        return coupling\n",
    "\n",
    "    def postprocess(self,method,para=None):\n",
    "        if method == 'origin':\n",
    "            y_pred=self.model.predict(np.array(self.df_test[self.var_list]))\n",
    "            tv = self.tv_origin\n",
    "        else:\n",
    "            if (method == 'unconstrained') or (method == 'partial'):\n",
    "                coupling = self.coupling_generator(method,para)\n",
    "                y_pred=postprocess(self.df_test,coupling,self.x_list,self.x_range,self.var_list,self.var_range,self.model,self.thresh)\n",
    "                tv=assess_tv(self.df_test,coupling,self.x_range,self.x_list,self.var_list)\n",
    "                if (para != None) and (method == 'partial'):\n",
    "                    method = method+'_'+str(para)\n",
    "            elif method == 'barycentre':\n",
    "                coupling = self.coupling_generator(method,para)\n",
    "                y_pred,tv=postprocess_bary(self.df_test,coupling,self.x_list,self.x_range,self.var_list,self.var_range,self.model,self.thresh)\n",
    "            else:\n",
    "                print('Unknown method')\n",
    "\n",
    "        di = DisparateImpact_postprocess(self.df_test,y_pred,favorable_label=self.favorable_label)\n",
    "        f1_macro = f1_score(self.df_test['Y'], y_pred, average='macro',sample_weight=self.df_test['W'])\n",
    "        f1_micro = f1_score(self.df_test['Y'], y_pred, average='micro',sample_weight=self.df_test['W'])\n",
    "        f1_weighted = f1_score(self.df_test['Y'], y_pred, average='weighted',sample_weight=self.df_test['W'])\n",
    "\n",
    "        new_row=pd.Series({'DI':di,'f1 macro':f1_macro,'f1 micro':f1_micro,'f1 weighted':f1_weighted,\n",
    "                           'TV distance':tv,'method':method})\n",
    "        return new_row.to_frame().T\n",
    "    \n",
    "    def thresh_generator(self):\n",
    "        num_thresh = 10\n",
    "        ba_arr = np.zeros(num_thresh)\n",
    "        ba_arr1 = np.zeros(num_thresh)\n",
    "        class_thresh_arr = np.linspace(0.1, 0.9, num_thresh)\n",
    "        coupling=self.coupling_generator('partial',para=1e-3)\n",
    "    \n",
    "        for idx, thresh in enumerate(class_thresh_arr):\n",
    "            y_pred=postprocess(self.df_test,coupling,self.x_list,self.x_range,self.var_list,self.var_range,self.model,thresh)\n",
    "            ba_arr[idx] = DisparateImpact_postprocess(self.df_test,y_pred,favorable_label=self.favorable_label)\n",
    "            ba_arr1[idx] = f1_score(self.df_test['Y'], y_pred, average='macro',sample_weight=self.df_test['W'])\n",
    "\n",
    "        best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
    "        best_thresh = class_thresh_arr[best_ind]\n",
    "        print(\"Optional threshold = \",class_thresh_arr)\n",
    "        print(\"Disparate Impact = \",ba_arr)\n",
    "        print(\"f1 scores = \",ba_arr1)\n",
    "        self.thresh = best_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path,var_list,pa):\n",
    "    column_names = ['age', 'workclass', 'fnlwgt', 'education',\n",
    "                'education-num', 'marital-status', 'occupation', 'relationship',\n",
    "                'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "                'native-country', 'Y']\n",
    "    na_values=['?']\n",
    "    pa_dict={'Male':1,'Female':0,'White':1,'Black':0}\n",
    "    label_dict={'>50K.':1,'>50K':1,'<=50K.':0,'<=50K':0}\n",
    "    train_path = os.path.join(data_path, 'adult.data')\n",
    "    test_path = os.path.join(data_path, 'adult.test')\n",
    "    train = pd.read_csv(train_path, header=None,names=column_names,\n",
    "                    skipinitialspace=True, na_values=na_values)\n",
    "    test = pd.read_csv(test_path, header=0,names=column_names,\n",
    "                    skipinitialspace=True, na_values=na_values)\n",
    "    messydata = pd.concat([test, train], ignore_index=True)[var_list+[pa,'Y']]\n",
    "    messydata=messydata.rename(columns={pa:'S'})\n",
    "    messydata['S']=messydata['S'].replace(pa_dict)\n",
    "    messydata['Y']=messydata['Y'].replace(label_dict)\n",
    "    messydata=messydata[(messydata['S']==0)|(messydata['S']==1)]\n",
    "    for col in var_list+['S','Y']:\n",
    "        messydata[col]=messydata[col].astype('int64')\n",
    "    messydata['W']=1\n",
    "    bins_capitalgain=[100,3500,7500,10000]\n",
    "    bins_capitalloss=[100,1600,1900,2200]\n",
    "    bins_age=[26,36,46,56]\n",
    "    bins_hours=[21,36,46,61]\n",
    "\n",
    "    messydata=categerise(messydata,'age',bins_age)\n",
    "    # messydata=categerise(messydata,'hours-per-week',bins_hours)\n",
    "    messydata=categerise(messydata,'capital-gain',bins_capitalgain)\n",
    "    messydata=categerise(messydata,'capital-loss',bins_capitalloss)\n",
    "    \n",
    "    return messydata\n",
    "\n",
    "def categerise(df,col,bins):\n",
    "    for i in range(len(bins)+1):\n",
    "        if i == 0:\n",
    "            df.loc[df[col] < bins[i], col] = i\n",
    "        elif i == len(bins):\n",
    "            df.loc[df[col] >= bins[i-1], col] = i\n",
    "        else:\n",
    "            df.loc[(df[col] >= bins[i-1])& (df[col] < bins[i]), col] = i        \n",
    "    return df\n",
    "\n",
    "def choose_x(var_list,messydata):\n",
    "    tv_dist=dict()\n",
    "    for x_name in var_list:\n",
    "        x_range_single=list(pd.pivot_table(messydata,index=x_name,values=['W'])[('W')].index) \n",
    "        dist=rdata_analysis(messydata,x_range_single,x_name)\n",
    "        tv_dist[x_name]=sum(abs(dist['x_0']-dist['x_1']))/2\n",
    "    x_list=[]\n",
    "    for key,val in tv_dist.items():\n",
    "        if val>0.1:\n",
    "            x_list+=[key]  \n",
    "    return x_list,tv_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: change this path\n",
    "data_path='C://personal//work//repair//.venv//Lib//site-packages//aif360//data//raw//adult'\n",
    "\n",
    "var_list=['age','capital-gain','capital-loss','education-num'] #'hours-per-week',\n",
    "pa='sex'\n",
    "favorable_label = 1\n",
    "var_dim=len(var_list)\n",
    "\n",
    "K=200\n",
    "e=0.01\n",
    "\n",
    "if pa == 'sex':\n",
    "    thresh=0.05\n",
    "elif pa == 'race':\n",
    "    thresh=0.1\n",
    "\n",
    "messydata = load_data(data_path,var_list,pa)\n",
    "x_list,tv_dist = choose_x(var_list,messydata)\n",
    "\n",
    "X=messydata[var_list+['S','W']].to_numpy() # [X,S,W]\n",
    "y=messydata['Y'].to_numpy() #[Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': np.float64(0.1010227688866829),\n",
       " 'capital-gain': np.float64(0.036924675713792855),\n",
       " 'capital-loss': np.float64(0.020068855964263464),\n",
       " 'education-num': np.float64(0.07095473385227195)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\personal\\work\\repair\\humancompatible\\repair\\group_blind_repair.py:297: RuntimeWarning: overflow encountered in exp\n",
      "  tmp.item(i, j) * V.item(i) * np.exp(-z * V.item(i)) for i in I\n",
      "c:\\personal\\work\\repair\\humancompatible\\repair\\group_blind_repair.py:300: RuntimeWarning: overflow encountered in exp\n",
      "  tmp.item(i, j) * (V.item(i) ** 2) * np.exp(-z * V.item(i)) for i in I\n",
      "c:\\personal\\work\\repair\\humancompatible\\repair\\group_blind_repair.py:318: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  b = a - fun(a) / dfun(a)\n",
      "c:\\personal\\work\\repair\\humancompatible\\repair\\data_analysis.py:28: RuntimeWarning: invalid value encountered in divide\n",
      "  rdist['x'] = np.array([pivot[i] for i in x_range]) / total\n",
      "c:\\personal\\work\\repair\\humancompatible\\repair\\data_analysis.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  rdist['x_0'] = np.array(\n",
      "c:\\personal\\work\\repair\\humancompatible\\repair\\data_analysis.py:48: RuntimeWarning: invalid value encountered in divide\n",
      "  rdist['x_1'] = np.array(\n"
     ]
    }
   ],
   "source": [
    "thresh=0.05\n",
    "x_list = ['age','education-num']\n",
    "methods=['origin','barycentre','partial']\n",
    "report=pd.DataFrame(columns=['DI','f1 macro','f1 micro','f1 weighted','TV distance','method'])\n",
    "for ignore in range(10):\n",
    "    # train val test 4:2:4\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3)\n",
    "    \n",
    "    clf=RandomForestClassifier(max_depth=5).fit(X_train[:,0:var_dim],y_train)\n",
    "    projpost = Projpostprocess(X_test,y_test,x_list,var_list,clf,K,e,thresh,favorable_label)\n",
    "    for method in methods[:-1]:\n",
    "        report = pd.concat([report,projpost.postprocess(method)], ignore_index=True)\n",
    "    \n",
    "    for p in [1e-2,1e-3,1e-4]:\n",
    "        report = pd.concat([report,projpost.postprocess('partial',para=p)], ignore_index=True)\n",
    "\n",
    "report.to_csv(path+'/data/E3_postprocess_adult_'+str(pa)+'.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DI</th>\n",
       "      <th>f1 macro</th>\n",
       "      <th>f1 micro</th>\n",
       "      <th>f1 weighted</th>\n",
       "      <th>TV distance</th>\n",
       "      <th>method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.421662</td>\n",
       "      <td>0.635702</td>\n",
       "      <td>0.809234</td>\n",
       "      <td>0.766005</td>\n",
       "      <td>0.131946</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.442585</td>\n",
       "      <td>0.640373</td>\n",
       "      <td>0.808568</td>\n",
       "      <td>0.767832</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.136757</td>\n",
       "      <td>0.569383</td>\n",
       "      <td>0.720223</td>\n",
       "      <td>0.701464</td>\n",
       "      <td>0.089953</td>\n",
       "      <td>partial_0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.802306</td>\n",
       "      <td>0.630994</td>\n",
       "      <td>0.781543</td>\n",
       "      <td>0.753144</td>\n",
       "      <td>0.027711</td>\n",
       "      <td>partial_0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.797469</td>\n",
       "      <td>0.642403</td>\n",
       "      <td>0.783129</td>\n",
       "      <td>0.758661</td>\n",
       "      <td>0.003762</td>\n",
       "      <td>partial_0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.461775</td>\n",
       "      <td>0.663315</td>\n",
       "      <td>0.814966</td>\n",
       "      <td>0.781645</td>\n",
       "      <td>0.136864</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.525756</td>\n",
       "      <td>0.661698</td>\n",
       "      <td>0.812663</td>\n",
       "      <td>0.780043</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.08952</td>\n",
       "      <td>0.5935</td>\n",
       "      <td>0.726877</td>\n",
       "      <td>0.715436</td>\n",
       "      <td>0.08676</td>\n",
       "      <td>partial_0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.070763</td>\n",
       "      <td>0.592971</td>\n",
       "      <td>0.714081</td>\n",
       "      <td>0.70924</td>\n",
       "      <td>0.028149</td>\n",
       "      <td>partial_0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.50286</td>\n",
       "      <td>0.67544</td>\n",
       "      <td>0.81727</td>\n",
       "      <td>0.786982</td>\n",
       "      <td>0.127928</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.534659</td>\n",
       "      <td>0.67469</td>\n",
       "      <td>0.815837</td>\n",
       "      <td>0.786091</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.050588</td>\n",
       "      <td>0.609932</td>\n",
       "      <td>0.733429</td>\n",
       "      <td>0.724037</td>\n",
       "      <td>0.091051</td>\n",
       "      <td>partial_0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.00486</td>\n",
       "      <td>0.610738</td>\n",
       "      <td>0.727389</td>\n",
       "      <td>0.721521</td>\n",
       "      <td>0.028041</td>\n",
       "      <td>partial_0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.025878</td>\n",
       "      <td>0.601106</td>\n",
       "      <td>0.715361</td>\n",
       "      <td>0.712093</td>\n",
       "      <td>0.003739</td>\n",
       "      <td>partial_0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.531482</td>\n",
       "      <td>0.67665</td>\n",
       "      <td>0.811998</td>\n",
       "      <td>0.783247</td>\n",
       "      <td>0.12462</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.507711</td>\n",
       "      <td>0.670955</td>\n",
       "      <td>0.802631</td>\n",
       "      <td>0.777018</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.944761</td>\n",
       "      <td>0.62783</td>\n",
       "      <td>0.739213</td>\n",
       "      <td>0.731574</td>\n",
       "      <td>0.082993</td>\n",
       "      <td>partial_0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.020932</td>\n",
       "      <td>0.600403</td>\n",
       "      <td>0.70021</td>\n",
       "      <td>0.702162</td>\n",
       "      <td>0.0255</td>\n",
       "      <td>partial_0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.119153</td>\n",
       "      <td>0.570033</td>\n",
       "      <td>0.685673</td>\n",
       "      <td>0.683653</td>\n",
       "      <td>0.003582</td>\n",
       "      <td>partial_0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.498546</td>\n",
       "      <td>0.674613</td>\n",
       "      <td>0.818549</td>\n",
       "      <td>0.788032</td>\n",
       "      <td>0.144719</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.494589</td>\n",
       "      <td>0.672981</td>\n",
       "      <td>0.815222</td>\n",
       "      <td>0.786012</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.995812</td>\n",
       "      <td>0.619222</td>\n",
       "      <td>0.750627</td>\n",
       "      <td>0.736453</td>\n",
       "      <td>0.092398</td>\n",
       "      <td>partial_0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.13389</td>\n",
       "      <td>0.58668</td>\n",
       "      <td>0.705021</td>\n",
       "      <td>0.702587</td>\n",
       "      <td>0.02894</td>\n",
       "      <td>partial_0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.084319</td>\n",
       "      <td>0.603729</td>\n",
       "      <td>0.722015</td>\n",
       "      <td>0.717194</td>\n",
       "      <td>0.003602</td>\n",
       "      <td>partial_0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.578626</td>\n",
       "      <td>0.675748</td>\n",
       "      <td>0.814301</td>\n",
       "      <td>0.786354</td>\n",
       "      <td>0.125325</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.603608</td>\n",
       "      <td>0.673393</td>\n",
       "      <td>0.810667</td>\n",
       "      <td>0.783886</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.979182</td>\n",
       "      <td>0.610572</td>\n",
       "      <td>0.746583</td>\n",
       "      <td>0.730669</td>\n",
       "      <td>0.0826</td>\n",
       "      <td>partial_0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.019832</td>\n",
       "      <td>0.596991</td>\n",
       "      <td>0.721861</td>\n",
       "      <td>0.714053</td>\n",
       "      <td>0.025893</td>\n",
       "      <td>partial_0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.977038</td>\n",
       "      <td>0.613454</td>\n",
       "      <td>0.72396</td>\n",
       "      <td>0.721304</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>partial_0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.575587</td>\n",
       "      <td>0.686139</td>\n",
       "      <td>0.815837</td>\n",
       "      <td>0.792146</td>\n",
       "      <td>0.133635</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.561842</td>\n",
       "      <td>0.681359</td>\n",
       "      <td>0.807698</td>\n",
       "      <td>0.786779</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.076646</td>\n",
       "      <td>0.6032</td>\n",
       "      <td>0.728669</td>\n",
       "      <td>0.720434</td>\n",
       "      <td>0.086434</td>\n",
       "      <td>partial_0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.122349</td>\n",
       "      <td>0.578486</td>\n",
       "      <td>0.665097</td>\n",
       "      <td>0.678877</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>partial_0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.927274</td>\n",
       "      <td>0.633695</td>\n",
       "      <td>0.730818</td>\n",
       "      <td>0.732798</td>\n",
       "      <td>0.003838</td>\n",
       "      <td>partial_0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.554304</td>\n",
       "      <td>0.673451</td>\n",
       "      <td>0.813226</td>\n",
       "      <td>0.784368</td>\n",
       "      <td>0.126016</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.559647</td>\n",
       "      <td>0.67287</td>\n",
       "      <td>0.81082</td>\n",
       "      <td>0.783159</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.037127</td>\n",
       "      <td>0.609905</td>\n",
       "      <td>0.72181</td>\n",
       "      <td>0.718377</td>\n",
       "      <td>0.084315</td>\n",
       "      <td>partial_0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.052236</td>\n",
       "      <td>0.589354</td>\n",
       "      <td>0.717152</td>\n",
       "      <td>0.708287</td>\n",
       "      <td>0.025883</td>\n",
       "      <td>partial_0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.042854</td>\n",
       "      <td>0.589654</td>\n",
       "      <td>0.711522</td>\n",
       "      <td>0.705753</td>\n",
       "      <td>0.004324</td>\n",
       "      <td>partial_0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.586029</td>\n",
       "      <td>0.681623</td>\n",
       "      <td>0.817219</td>\n",
       "      <td>0.791237</td>\n",
       "      <td>0.129553</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.599886</td>\n",
       "      <td>0.681286</td>\n",
       "      <td>0.816451</td>\n",
       "      <td>0.790784</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.988457</td>\n",
       "      <td>0.615534</td>\n",
       "      <td>0.749347</td>\n",
       "      <td>0.735195</td>\n",
       "      <td>0.091287</td>\n",
       "      <td>partial_0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.149583</td>\n",
       "      <td>0.575751</td>\n",
       "      <td>0.69678</td>\n",
       "      <td>0.695296</td>\n",
       "      <td>0.027585</td>\n",
       "      <td>partial_0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.128949</td>\n",
       "      <td>0.568501</td>\n",
       "      <td>0.672621</td>\n",
       "      <td>0.680324</td>\n",
       "      <td>0.003708</td>\n",
       "      <td>partial_0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.54287</td>\n",
       "      <td>0.675321</td>\n",
       "      <td>0.813124</td>\n",
       "      <td>0.783686</td>\n",
       "      <td>0.134334</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.529317</td>\n",
       "      <td>0.667618</td>\n",
       "      <td>0.807033</td>\n",
       "      <td>0.7779</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.084022</td>\n",
       "      <td>0.596381</td>\n",
       "      <td>0.726621</td>\n",
       "      <td>0.713841</td>\n",
       "      <td>0.086347</td>\n",
       "      <td>partial_0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.084991</td>\n",
       "      <td>0.587559</td>\n",
       "      <td>0.674617</td>\n",
       "      <td>0.684637</td>\n",
       "      <td>0.027317</td>\n",
       "      <td>partial_0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.901395</td>\n",
       "      <td>0.635037</td>\n",
       "      <td>0.731535</td>\n",
       "      <td>0.73118</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>partial_0.0001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          DI  f1 macro  f1 micro f1 weighted TV distance          method\n",
       "0   0.421662  0.635702  0.809234    0.766005    0.131946          origin\n",
       "1   0.442585  0.640373  0.808568    0.767832    0.000215      barycentre\n",
       "2   1.136757  0.569383  0.720223    0.701464    0.089953    partial_0.01\n",
       "3   0.802306  0.630994  0.781543    0.753144    0.027711   partial_0.001\n",
       "4   0.797469  0.642403  0.783129    0.758661    0.003762  partial_0.0001\n",
       "5   0.461775  0.663315  0.814966    0.781645    0.136864          origin\n",
       "6   0.525756  0.661698  0.812663    0.780043    0.000961      barycentre\n",
       "7    1.08952    0.5935  0.726877    0.715436     0.08676    partial_0.01\n",
       "8   1.070763  0.592971  0.714081     0.70924    0.028149   partial_0.001\n",
       "10   0.50286   0.67544   0.81727    0.786982    0.127928          origin\n",
       "11  0.534659   0.67469  0.815837    0.786091    0.000996      barycentre\n",
       "12  1.050588  0.609932  0.733429    0.724037    0.091051    partial_0.01\n",
       "13   1.00486  0.610738  0.727389    0.721521    0.028041   partial_0.001\n",
       "14  1.025878  0.601106  0.715361    0.712093    0.003739  partial_0.0001\n",
       "15  0.531482   0.67665  0.811998    0.783247     0.12462          origin\n",
       "16  0.507711  0.670955  0.802631    0.777018    0.000997      barycentre\n",
       "17  0.944761   0.62783  0.739213    0.731574    0.082993    partial_0.01\n",
       "18  1.020932  0.600403   0.70021    0.702162      0.0255   partial_0.001\n",
       "19  1.119153  0.570033  0.685673    0.683653    0.003582  partial_0.0001\n",
       "20  0.498546  0.674613  0.818549    0.788032    0.144719          origin\n",
       "21  0.494589  0.672981  0.815222    0.786012    0.000037      barycentre\n",
       "22  0.995812  0.619222  0.750627    0.736453    0.092398    partial_0.01\n",
       "23   1.13389   0.58668  0.705021    0.702587     0.02894   partial_0.001\n",
       "24  1.084319  0.603729  0.722015    0.717194    0.003602  partial_0.0001\n",
       "25  0.578626  0.675748  0.814301    0.786354    0.125325          origin\n",
       "26  0.603608  0.673393  0.810667    0.783886    0.000533      barycentre\n",
       "27  0.979182  0.610572  0.746583    0.730669      0.0826    partial_0.01\n",
       "28  1.019832  0.596991  0.721861    0.714053    0.025893   partial_0.001\n",
       "29  0.977038  0.613454   0.72396    0.721304    0.003968  partial_0.0001\n",
       "30  0.575587  0.686139  0.815837    0.792146    0.133635          origin\n",
       "31  0.561842  0.681359  0.807698    0.786779    0.000934      barycentre\n",
       "32  1.076646    0.6032  0.728669    0.720434    0.086434    partial_0.01\n",
       "33  1.122349  0.578486  0.665097    0.678877     0.02731   partial_0.001\n",
       "34  0.927274  0.633695  0.730818    0.732798    0.003838  partial_0.0001\n",
       "35  0.554304  0.673451  0.813226    0.784368    0.126016          origin\n",
       "36  0.559647   0.67287   0.81082    0.783159    0.000206      barycentre\n",
       "37  1.037127  0.609905   0.72181    0.718377    0.084315    partial_0.01\n",
       "38  1.052236  0.589354  0.717152    0.708287    0.025883   partial_0.001\n",
       "39  1.042854  0.589654  0.711522    0.705753    0.004324  partial_0.0001\n",
       "40  0.586029  0.681623  0.817219    0.791237    0.129553          origin\n",
       "41  0.599886  0.681286  0.816451    0.790784    0.000331      barycentre\n",
       "42  0.988457  0.615534  0.749347    0.735195    0.091287    partial_0.01\n",
       "43  1.149583  0.575751   0.69678    0.695296    0.027585   partial_0.001\n",
       "44  1.128949  0.568501  0.672621    0.680324    0.003708  partial_0.0001\n",
       "45   0.54287  0.675321  0.813124    0.783686    0.134334          origin\n",
       "46  0.529317  0.667618  0.807033      0.7779    0.000831      barycentre\n",
       "47  1.084022  0.596381  0.726621    0.713841    0.086347    partial_0.01\n",
       "48  1.084991  0.587559  0.674617    0.684637    0.027317   partial_0.001\n",
       "49  0.901395  0.635037  0.731535     0.73118    0.003774  partial_0.0001"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         DI  f1 macro  f1 micro f1 weighted TV distance  method\n",
      "0  0.484025  0.655927  0.809797    0.773782    0.127735  origin\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance      method\n",
      "0  0.535226  0.645612  0.806726    0.768004    0.001148  barycentre\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance        method\n",
      "0  0.514857  0.642163  0.805958    0.766168    0.088536  partial_0.01\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance         method\n",
      "0  0.531562  0.639331  0.804422    0.764317    0.028334  partial_0.001\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance  method\n",
      "0  0.604063  0.681044  0.815632    0.790371    0.127107  origin\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance      method\n",
      "0  0.590638  0.663966  0.812612    0.781897    0.000984  barycentre\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance        method\n",
      "0  0.647149  0.663946  0.809234     0.78054    0.084418  partial_0.01\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance         method\n",
      "0  0.649411  0.663891  0.808773    0.780332    0.028338  partial_0.001\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance  method\n",
      "0  0.543698  0.673298  0.818908    0.789703    0.132636  origin\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance      method\n",
      "0  0.582315  0.664767  0.816502    0.785137    0.000193  barycentre\n",
      "        DI  f1 macro  f1 micro f1 weighted TV distance        method\n",
      "0  0.61326  0.652457  0.811588    0.777968    0.090623  partial_0.01\n",
      "         DI  f1 macro f1 micro f1 weighted TV distance         method\n",
      "0  0.624803  0.647177  0.80995    0.775077    0.027056  partial_0.001\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance  method\n",
      "0  0.538763  0.673213  0.811998     0.78297    0.141817  origin\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance      method\n",
      "0  0.590039  0.662825  0.809234    0.777334    0.000039  barycentre\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance        method\n",
      "0  0.573222  0.659489  0.808312    0.775508     0.09445  partial_0.01\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance         method\n",
      "0  0.582558  0.655783  0.806879    0.773319     0.02702  partial_0.001\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance  method\n",
      "0  0.552007  0.681579  0.817321    0.790196    0.134432  origin\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance      method\n",
      "0  0.550678  0.678516  0.815478    0.788144     0.00003  barycentre\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance        method\n",
      "0  0.571881  0.670898  0.814557    0.784496    0.094802  partial_0.01\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance         method\n",
      "0  0.593913  0.665744  0.811947    0.781237    0.025813  partial_0.001\n"
     ]
    }
   ],
   "source": [
    "thresh=0.3\n",
    "x_list = ['age','education-num']\n",
    "methods=['origin','barycentre','partial'] # Place ROC in the end\n",
    "report=pd.DataFrame(columns=['DI','f1 macro','f1 micro','f1 weighted','TV distance','method'])\n",
    "for ignore in range(5):\n",
    "    # train val test 4:2:4\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "    # X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3)\n",
    "\n",
    "    clf=RandomForestClassifier(max_depth=5).fit(X_train[:,0:var_dim],y_train)\n",
    "    projpost = Projpostprocess(X_test,y_test,x_list,var_list,clf,K,e,thresh,favorable_label)\n",
    "    \n",
    "    print(projpost.postprocess('origin'))\n",
    "    print(projpost.postprocess('barycentre'))\n",
    "    for t in range(2,4):\n",
    "        print(projpost.postprocess('partial',para=10**(-t)))\n",
    "\n",
    "# report.to_csv(path+'/data/E3_postprocess_adult_'+str(pa)+'.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optional threshold =  [0.1        0.18888889 0.27777778 0.36666667 0.45555556 0.54444444\n",
      " 0.63333333 0.72222222 0.81111111 0.9       ]\n",
      "Disparate Impact =  [0.65875427 0.65678285 0.65782143 0.65056999 0.65547625 0.64543255\n",
      " 0.65870031 0.66293632 0.66293632 0.65876567]\n",
      "f1 scores =  [0.6742306  0.67411561 0.66850933 0.66836019 0.6681131  0.66342243\n",
      " 0.65872396 0.65751368 0.65751368 0.63291156]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.7222222222222222)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valpost = Projpostprocess(X_val,y_val,x_list,var_list,clf,K,e,'auto')\n",
    "valpost.thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal classification threshold (with fairness constraints) = 0.2900\n",
      "Optimal ROC margin = 0.0322\n",
      "Optimal classification threshold (with fairness constraints) = 0.1700\n",
      "Optimal ROC margin = 0.0378\n",
      "Optimal classification threshold (with fairness constraints) = 0.3100\n",
      "Optimal ROC margin = 0.0344\n",
      "Optimal classification threshold (with fairness constraints) = 0.1700\n",
      "Optimal ROC margin = 0.0189\n",
      "Optimal classification threshold (with fairness constraints) = 0.1900\n",
      "Optimal ROC margin = 0.0211\n",
      "Optimal classification threshold (with fairness constraints) = 0.1900\n",
      "Optimal ROC margin = 0.0211\n",
      "Optimal classification threshold (with fairness constraints) = 0.1700\n",
      "Optimal ROC margin = 0.0189\n",
      "Optimal classification threshold (with fairness constraints) = 0.1700\n",
      "Optimal ROC margin = 0.0189\n",
      "Optimal classification threshold (with fairness constraints) = 0.1900\n",
      "Optimal ROC margin = 0.0211\n",
      "Optimal classification threshold (with fairness constraints) = 0.2900\n",
      "Optimal ROC margin = 0.0322\n"
     ]
    }
   ],
   "source": [
    "methods=['origin','unconstrained','barycentre','partial','ROC'] # Place ROC in the end\n",
    "report=pd.DataFrame(columns=['DI','f1 macro','f1 micro','f1 weighted','TV distance','method'])\n",
    "for ignore in range(10):\n",
    "    # train val test 4:2:4\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3)\n",
    "\n",
    "    clf=RandomForestClassifier(max_depth=5).fit(X_train[:,0:var_dim],y_train)\n",
    "    projpost = Projpostprocess(X_test,y_test,x_list,var_list,clf,K,e,thresh,favorable_label)\n",
    "    for method in methods[:-1]:\n",
    "        # report = pd.concat([report,projpost.postprocess(method,para=1e-2)], ignore_index=True)\n",
    "        report = pd.concat([report,projpost.postprocess(method,para=1e-3)], ignore_index=True)\n",
    "\n",
    "    ROCpost = ROCpostprocess(X_val,y_val,var_list,clf,favorable_label) # use validation set to train a ROC model\n",
    "    report = pd.concat([report,ROCpost.postprocess(X_test,y_test,tv_origin=projpost.tv_origin)], ignore_index=True)\n",
    "\n",
    "report.to_csv(path+'/data/report_postprocess_adult_'+str(pa)+'.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': np.float64(0.04744857663969924),\n",
       " 'education-num': np.float64(0.056762405582129784),\n",
       " 'capital-gain': np.float64(0.021127950774052707),\n",
       " 'capital-loss': np.float64(0.011363681253224836),\n",
       " 'hours-per-week': np.float64(0.04445283428803036)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa = 'race'\n",
    "label_map = {1.0: '>50K', 0.0: '<=50K'}\n",
    "privileged_groups = [{pa: 1}]\n",
    "unprivileged_groups = [{pa: 0}]\n",
    "if pa == 'sex':\n",
    "    thresh=0.05\n",
    "    protected_attribute_maps = [{1.0: 'Male', 0.0: 'Female'}]\n",
    "    cd = AdultDataset(protected_attribute_names=[pa],privileged_classes=[['Male'],[1.0]], \n",
    "        metadata={'label_map': label_map,'protected_attribute_maps': protected_attribute_maps},\n",
    "        # categorical_features=['workclass', 'marital-status', 'occupation', 'relationship', 'native-country']\n",
    "        features_to_drop=['race','fnlwgt','education','relationship',\n",
    "                          'native-country','workclass','marital-status','occupation'])\n",
    "elif pa == 'race':\n",
    "    thresh=0.1\n",
    "    protected_attribute_maps = [{1.0: 'White', 0.0:'Non-white'}]\n",
    "    cd = AdultDataset(protected_attribute_names=[pa],privileged_classes=[['White'],[1.0]],\n",
    "        metadata={'label_map': label_map,'protected_attribute_maps': protected_attribute_maps}, #\n",
    "        features_to_drop=['sex','fnlwgt','education','relationship',\n",
    "                          'native-country','workclass','marital-status','occupation'])\n",
    "    #,'workclass','marital-status','occupation','relationship',\n",
    "\n",
    "# train,test = cd.split([0.6], shuffle=True) #len(test.instance_names) = 2057\n",
    "var_list = cd.feature_names.copy()\n",
    "var_list.remove(pa)\n",
    "var_dim=len(var_list)\n",
    "\n",
    "K=200\n",
    "e=0.01\n",
    "bins_capitalgain=[100,3500,7500,10000]\n",
    "bins_capitalloss=[100,1600,1900,2200]\n",
    "\n",
    "messydata=cd.convert_to_dataframe()[0]\n",
    "messydata=messydata.rename(columns={pa:'S',cd.label_names[0]:'Y'})\n",
    "messydata=messydata[(messydata['S']==1)|(messydata['S']==0)]\n",
    "for col in var_list+['S','Y']:\n",
    "    messydata[col]=messydata[col].astype('int64')\n",
    "messydata['W']=cd.instance_weights\n",
    "# project 0-100 to {0,1,...,5}\n",
    "messydata['age']=np.floor((messydata['age'].to_numpy()-17)/15)\n",
    "messydata['hours-per-week']=np.floor(messydata['hours-per-week'].to_numpy()/20)\n",
    "messydata=categerise(messydata,'capital-gain',bins_capitalgain)\n",
    "messydata=categerise(messydata,'capital-loss',bins_capitalloss)\n",
    "\n",
    "X=messydata[var_list+['S','W']].to_numpy() # [X,S,W]\n",
    "y=messydata['Y'].to_numpy() #[Y]\n",
    "tv_dist=dict()\n",
    "for x_name in var_list:\n",
    "    x_range_single=list(pd.pivot_table(messydata,index=x_name,values=['W'])[('W')].index) \n",
    "    dist=rdata_analysis(messydata,x_range_single,x_name)\n",
    "    tv_dist[x_name]=sum(abs(dist['x_0']-dist['x_1']))/2\n",
    "x_list=[]\n",
    "for key,val in tv_dist.items():\n",
    "    if val>0.045:\n",
    "        x_list+=[key]        \n",
    "tv_dist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
