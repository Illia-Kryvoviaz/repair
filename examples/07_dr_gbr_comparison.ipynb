{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c874a8c",
   "metadata": {},
   "source": [
    "# DR vs GBR: Distributional Repair & Group-Blind Repair (Adult dataset)\n",
    "\n",
    "This notebook compares two dataset-repair approaches:\n",
    "\n",
    "- **Distributional Repair (DR)** group-aware: learns two OT maps (S=0 and S=1) to a shared barycenter (conditional on U).\n",
    "\n",
    "- **GroupBlind Repair (GBR)** group-blind: learns one OT map from the pooled distribution to a target (we will align this target with DR’s barycenter so the comparison is meaningful). Variants:\n",
    "\n",
    "    - `baseline` (no fairness vector)\n",
    "\n",
    "    - `partial` / `total` (use fairness vector *V*), optional.\n",
    "\n",
    "**Data:** Adult (AIF360).  \n",
    "**Protected (S):** sex (0 = Female, 1 = Male)  \n",
    "**Unprotected (U):** college_educated (0/1)  \n",
    "**Features (X):** age, hours-per-week (continuous)\n",
    "\n",
    "We compute U-Mean KLD (average KL divergence between P(X|S=1,U) and P(X|S=0,U), averaged over *U*) on:\n",
    "\n",
    "- **Research** split (used to learn transport)\n",
    "\n",
    "- **Archive** split (held-out to check generalization)\n",
    "\n",
    "Smaller values indicate closer group distributions conditional on *U* (0 = identical)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af12c022",
   "metadata": {},
   "source": [
    "## Imports & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ae5db0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from aif360.datasets import AdultDataset\n",
    "\n",
    "from humancompatible.repair.distributional_repair import DistributionalRepair\n",
    "from humancompatible.repair.group_blind_repair import GroupBlindRepair\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9042f3",
   "metadata": {},
   "source": [
    "## Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdc67050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_adult_dataset(s,u,x,y):\n",
    "    def custom_preprocessing(df):\n",
    "        pd.set_option('future.no_silent_downcasting', True)\n",
    "        def group_race(x):\n",
    "            if x == \"White\":\n",
    "                return 1.0\n",
    "            else:\n",
    "                return 0.0\n",
    "\n",
    "        df['race'] = df['race'].apply(lambda x: group_race(x))\n",
    "\n",
    "        # Encode 'sex' column as numerical values\n",
    "        df['sex'] = df['sex'].map({'Female': 0.0, 'Male': 1.0})\n",
    "\n",
    "        df['Income Binary'] = df['income-per-year']\n",
    "        df['Income Binary'] = df['Income Binary'].replace(to_replace='>50K.', value=1, regex=True)\n",
    "        df['Income Binary'] = df['Income Binary'].replace(to_replace='>50K', value=1, regex=True)\n",
    "        df['Income Binary'] = df['Income Binary'].replace(to_replace='<=50K.', value=0, regex=True)\n",
    "        df['Income Binary'] = df['Income Binary'].replace(to_replace='<=50K', value=0, regex=True)\n",
    "        # 1 if education-num is greater than 9, 0 otherwise\n",
    "        df['college_educated'] = (df['education-num'] > 9).astype(int)\n",
    "\n",
    "        #drop nan columns\n",
    "        df = df.dropna()\n",
    "\n",
    "        return df\n",
    "\n",
    "    adult = AdultDataset(\n",
    "        label_name=y,\n",
    "        favorable_classes=[1,1],\n",
    "        protected_attribute_names=[s],\n",
    "        privileged_classes=[[1.0]],\n",
    "        instance_weights_name=None,\n",
    "        categorical_features=[],\n",
    "        features_to_keep=[s]+[u]+x,\n",
    "        na_values=[],\n",
    "        custom_preprocessing=custom_preprocessing,\n",
    "        features_to_drop=[],\n",
    "        metadata={}\n",
    "    )\n",
    "    return adult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cc88cc",
   "metadata": {},
   "source": [
    "## KLD (Kullback-Leibler Divergence) evaluation helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbe2c3f",
   "metadata": {},
   "source": [
    "We use a simple distribution-level score to quantify how much group signal remains in each feature after repair. \n",
    "\n",
    "For each value of U, we take the feature X restricted to the two groups (S=0 and S=1), fit 1-D Gaussian KDEs for each group on a shared grid of 500 points spanning the pooled min/max, add a tiny *ε* to avoid zeros, and compute a one-directional Kullback-Leibler divergence KL(P0||P1) (i.e., how well the S=1 density explains S=0). \n",
    "\n",
    "We then weight that KL by the prevalence of the corresponding U value and average across all U values. Lower values mean the conditional distributions P(X|S,U) are closer (0 means identical). \n",
    "\n",
    "Because KL is asymmetric, the score reflects the fixed direction S=0 -> S=1; we keep this direction consistent across runs so results are comparable. U values with missing group samples are skipped. The absolute scale depends on the KDE bandwidth and grid resolution, so the metric is best used for before/after comparisons rather than as an absolute number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74ea1232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval_kld(x_0, x_1):\n",
    "    support = np.linspace(np.min([np.min(x_0), np.min(x_1)]), np.max([np.max(x_0), np.max(x_1)]), 500).reshape(-1,1)\n",
    "    kde_0 = KernelDensity(kernel='gaussian',bandwidth='silverman').fit(x_0.reshape(-1,1))\n",
    "    pmf_0 = np.exp(kde_0.score_samples(support)) \n",
    "    #add a small value to avoid division by zero\n",
    "    pmf_0 += 1e-10\n",
    "    kde_1 = KernelDensity(kernel='gaussian',bandwidth='silverman').fit(x_1.reshape(-1,1))\n",
    "    pmf_1 = np.exp(kde_1.score_samples(support))\n",
    "    pmf_1 += 1e-10\n",
    "    return - np.sum(pmf_0 * np.log(pmf_1 / pmf_0))\n",
    "\n",
    "def eval_kld(x, s, u, order=[0,1]):\n",
    "    tot_kld = 0.0\n",
    "    for u_val, u_count in u.value_counts().items():\n",
    "        mask_0 = np.asarray((u == u_val) & (s == 0))\n",
    "        mask_1 = np.asarray((u == u_val) & (s == 1))\n",
    "        if (np.sum(mask_0) == 0) or (np.sum(mask_1) == 0):\n",
    "            continue\n",
    "        tmp = _eval_kld(x[mask_0].values, x[mask_1].values)\n",
    "        if np.isnan(tmp):\n",
    "            continue\n",
    "        tot_kld += tmp * u_count / len(u)\n",
    "    return tot_kld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140d29ed",
   "metadata": {},
   "source": [
    "## Parameters for Distributional Repair Algorithm\n",
    "\n",
    "- `n_q` is the number of support points for the probability distribution function. This defaults to 250, however this can be increased to improve the accuracy of the repair algorithm.\n",
    "  \n",
    "- `S` is the protected or sensitive attribute for which the repair is against\n",
    "\n",
    "- `U` the name of the unprotected attribute. This should not be a sensitive attribute, or be used later by a model to predict the outcome.\n",
    "\n",
    "- `X` is a list of features used by the model to make its predictions. This should not include the sensitive attribute.\n",
    "\n",
    "- `X_continuous` is a list of continuous features in `X`\n",
    "\n",
    "- `Y` is the outcome of a model, which is predicted using the features in `X`. This should be a binary outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8de7069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_R = 10000 # number of points in the research dataset\n",
    "n_q = 250 # number of supports under the estimated pdfs\n",
    "\n",
    "S = 'sex'\n",
    "U = 'college_educated'\n",
    "X = ['age','hours-per-week']\n",
    "X_continuous= ['age','hours-per-week']\n",
    "Y = 'Income Binary'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db297f8",
   "metadata": {},
   "source": [
    "## Evaluating the DR method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fabd225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 5) (38842, 5)\n"
     ]
    }
   ],
   "source": [
    "data = load_adult_dataset(S, U, X, Y)\n",
    "dataset_R, dataset_A = data.split([n_R], shuffle=True)\n",
    "\n",
    "df_R = dataset_R.convert_to_dataframe()[0]\n",
    "df_A = dataset_A.convert_to_dataframe()[0]\n",
    "\n",
    "print(df_R.shape, df_A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6a4b1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DR done in 7.6s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "dist_repair = DistributionalRepair(S, U, X, Y, X_continuous, n_q)\n",
    "dist_repair.fit(dataset_R)\n",
    "\n",
    "dataset_A_repaired = dist_repair.transform(dataset_A)\n",
    "dataset_R_repaired = dist_repair.transform(dataset_R)\n",
    "t_dr = time.time() - t0\n",
    "print(f\"DR done in {t_dr:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92615859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DR - Archive (orig, repaired): [0.525656468515266, 1.1325513156755966] [0.4412919681225086, 0.26763968451774955]\n",
      "DR - Research (orig, repaired): [0.7031282082550464, 1.2129718230104007] [0.3008853718766283, 0.3159615783207155]\n"
     ]
    }
   ],
   "source": [
    "tilde_x_R = dataset_R_repaired.convert_to_dataframe()[0][X]\n",
    "tilde_x_A = dataset_A_repaired.convert_to_dataframe()[0][X]\n",
    "\n",
    "kld_x_R = np.zeros((len(X), 2))\n",
    "kld_x_tilde_R = np.zeros((len(X), 2))\n",
    "kld_x_A = np.zeros((len(X), 2))\n",
    "kld_x_tilde_A = np.zeros((len(X), 2))\n",
    "\n",
    "for i, feat in enumerate(X):\n",
    "    # Research (original vs repaired)\n",
    "    kld_x_R[i, 0] = eval_kld(dist_repair.x_R[feat], dist_repair.s_R, dist_repair.u_R, order=[0, 1])\n",
    "    kld_x_R[i, 1] = eval_kld(dist_repair.x_R[feat], dist_repair.s_R, dist_repair.u_R, order=[1, 0])\n",
    "    kld_x_tilde_R[i, 0] = eval_kld(tilde_x_R[feat], dist_repair.s_R, dist_repair.u_R, order=[0, 1])\n",
    "    kld_x_tilde_R[i, 1] = eval_kld(tilde_x_R[feat], dist_repair.s_R, dist_repair.u_R, order=[1, 0])\n",
    "\n",
    "    # Archive (hold-out)\n",
    "    kld_x_A[i, 0] = eval_kld(df_A[feat], df_A[S], df_A[U], order=[0, 1])\n",
    "    kld_x_A[i, 1] = eval_kld(df_A[feat], df_A[S], df_A[U], order=[1, 0])\n",
    "    kld_x_tilde_A[i, 0] = eval_kld(tilde_x_A[feat], df_A[S], df_A[U], order=[0, 1])\n",
    "    kld_x_tilde_A[i, 1] = eval_kld(tilde_x_A[feat], df_A[S], df_A[U], order=[1, 0])\n",
    "\n",
    "kld_x_A_mean = np.mean(kld_x_A, axis=1)\n",
    "kld_x_tilde_A_mean = np.mean(kld_x_tilde_A, axis=1)\n",
    "kld_x_R_mean = np.mean(kld_x_R, axis=1)\n",
    "kld_x_tilde_R_mean = np.mean(kld_x_tilde_R, axis=1)\n",
    "\n",
    "print(\"DR - Archive (orig, repaired):\", kld_x_A_mean.tolist(), kld_x_tilde_A_mean.tolist())\n",
    "print(\"DR - Research (orig, repaired):\", kld_x_R_mean.tolist(), kld_x_tilde_R_mean.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
